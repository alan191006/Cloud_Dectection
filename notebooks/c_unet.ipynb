{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20241440",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from pthflops import count_ops\n",
    "from torchvision import transforms\n",
    "from prettytable import PrettyTable\n",
    "from matplotlib.pyplot import imshow\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfefaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774fc1af",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c380e",
   "metadata": {},
   "source": [
    "### Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RED_PATH = r\"C:\\Users\\hdmqu\\Documents\\LUX Aerobot\\Dataset\\38-Cloud_training\\train_red\" + \"\\\\\"\n",
    "GREEN_PATH = r\"C:\\Users\\hdmqu\\Documents\\LUX Aerobot\\Dataset\\38-Cloud_training\\train_green\" + \"\\\\\"\n",
    "BLUE_PATH = r\"C:\\Users\\hdmqu\\Documents\\LUX Aerobot\\Dataset\\38-Cloud_training\\train_blue\" + \"\\\\\"\n",
    "NIR_PATH = r\"C:\\Users\\hdmqu\\Documents\\LUX Aerobot\\Dataset\\38-Cloud_training\\train_nir\" + \"\\\\\"\n",
    "GT_PATH = r\"C:\\Users\\hdmqu\\Documents\\LUX Aerobot\\Dataset\\38-Cloud_training\\train_gt\" + \"\\\\\"\n",
    "\n",
    "red_images = os.listdir(RED_PATH)\n",
    "green_images = os.listdir(GREEN_PATH)\n",
    "blue_images = os.listdir(BLUE_PATH)\n",
    "nir_images = os.listdir(NIR_PATH)\n",
    "gt_images = os.listdir(NIR_PATH)\n",
    "\n",
    "print(f'The folder contains {len(red_images)} images.')\n",
    "\n",
    "image_path = random.choice(red_images)\n",
    "red_image = Image.open(RED_PATH + image_path)\n",
    "green_image = Image.open(GREEN_PATH + image_path.replace(\"red\", \"green\"))\n",
    "blue_image = Image.open(BLUE_PATH + image_path.replace(\"red\", \"blue\"))\n",
    "nir_image = Image.open(NIR_PATH + image_path.replace(\"red\", \"nir\"))\n",
    "gt_image = Image.open(GT_PATH + image_path.replace(\"red\", \"gt\"))\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(18, 18))\n",
    "axs[0].imshow(red_image, cmap='gray')\n",
    "axs[0].set_title('Red')\n",
    "axs[1].imshow(green_image, cmap='gray')\n",
    "axs[1].set_title('Green')\n",
    "axs[2].imshow(blue_image, cmap='gray')\n",
    "axs[2].set_title('Blue')\n",
    "axs[3].imshow(nir_image, cmap='gray')\n",
    "axs[3].set_title('NIR')\n",
    "axs[4].imshow(gt_image, cmap='gray')\n",
    "axs[4].set_title('Mask')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ed2fa",
   "metadata": {},
   "source": [
    "### Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(gt_image)\n",
    "print(f'Mask shape {mask.shape}')\n",
    "print(f'Min = {mask.min()} , Max = {mask.max()}')\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048833db",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5857bde0",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = int(len(red_images) * 0.7)\n",
    "valid_length = int(len(red_images) * 0.2)\n",
    "test_length = int(len(red_images) * 0.2)\n",
    "\n",
    "images = np.split(red_images, [train_length, train_length + valid_length, len(red_images)])\n",
    "train_images = images[0]\n",
    "valid_images = images[1]\n",
    "test_images = images[2]\n",
    "\n",
    "print(f'Train images: {len(train_images)}')\n",
    "print(f'Valid images: {len(valid_images)}')\n",
    "print(f'Test  images: {len(test_images)}')\n",
    "print(f'Total images: {len(train_images) + len(valid_images) + len(test_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74bc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "valid_list = []\n",
    "test_list = []\n",
    "\n",
    "r_dir = Path(RED_PATH)\n",
    "\n",
    "for file_name in train_images:\n",
    "    train_list.append(RED_PATH + file_name)\n",
    "        \n",
    "for file_name in valid_images:\n",
    "    valid_list.append(RED_PATH + file_name)\n",
    "\n",
    "for file_name in test_images:\n",
    "    test_list.append(RED_PATH + file_name)\n",
    "\n",
    "images = (train_list, valid_list, test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd113c",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, r_dir, g_dir, b_dir, nir_dir, gt_dir, file_list):\n",
    "        super().__init__()\n",
    "        self.files = [self.combine_files(f, g_dir, b_dir, nir_dir, gt_dir) for f in file_list if not os.path.isdir(f)]\n",
    "        \n",
    "    def combine_files(self, r_file: Path, g_dir, b_dir, nir_dir, gt_dir):\n",
    "        files = {\n",
    "            'red': r_file, \n",
    "            'green': r_file.replace('red', 'green'),\n",
    "            'blue': r_file.replace('red', 'blue'), \n",
    "            'nir': r_file.replace('red', 'nir'),\n",
    "            'gt': r_file.replace('red', 'gt')\n",
    "        }\n",
    "        return files\n",
    "                                       \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "     \n",
    "    def open_as_array(self, idx, invert=False, include_nir=False):\n",
    "        raw_rgb = np.stack([\n",
    "            np.array(Image.open(self.files[idx]['red'])),\n",
    "            np.array(Image.open(self.files[idx]['green'])),\n",
    "            np.array(Image.open(self.files[idx]['blue'])),\n",
    "        ], axis=2)\n",
    "        if include_nir:\n",
    "            nir = np.expand_dims(np.array(Image.open(self.files[idx]['nir'])), 2)\n",
    "            raw_rgb = np.concatenate([raw_rgb, nir], axis=2)\n",
    "        if invert:\n",
    "            raw_rgb = raw_rgb.transpose((2,0,1))\n",
    "        return (raw_rgb / np.iinfo(raw_rgb.dtype).max) # normalized\n",
    "\n",
    "\n",
    "    def open_mask(self, idx, add_dims=False):\n",
    "        raw_mask = np.array(Image.open(self.files[idx]['gt']))\n",
    "        raw_mask = np.where(raw_mask==255, 1, 0)\n",
    "        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.open_as_array(idx, invert=True, include_nir=True), dtype=torch.float32)\n",
    "        y = torch.tensor(self.open_mask(idx, add_dims=False), dtype=torch.long)\n",
    "        return x, y\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Dataset class with {self.__len__()} files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CloudDataset(RED_PATH, GREEN_PATH, BLUE_PATH, NIR_PATH, GT_PATH, images[0])\n",
    "valid_dataset = CloudDataset(RED_PATH, GREEN_PATH, BLUE_PATH, NIR_PATH, GT_PATH, images[1])\n",
    "test_dataset = CloudDataset(RED_PATH, GREEN_PATH, BLUE_PATH, NIR_PATH, GT_PATH, images[2])\n",
    "\n",
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a12bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input example\n",
    "x, y = train_dataset[0]\n",
    "print(f'Image: {x.shape}')\n",
    "print(f'Mask: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b28da",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd34824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKER = 0\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {DEVICE} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    batch = list(filter (lambda x:torch.sum(x[0]).item() != 0, batch))\n",
    "    if len(batch) == 0:\n",
    "        print(\"DOOOOO\")\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKER, collate_fn=my_collate)\n",
    "valid_loader = DataLoader(valid_dataset, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKER, collate_fn=my_collate)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKER, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d46778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View batch\n",
    "for images, labels in train_loader:\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid(images, nrow=4).permute(1, 2, 0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8be90",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a14d1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47657755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class depthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super(depthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=3, padding=1, groups=nin)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = self.contract_block(in_channels, 32)\n",
    "        self.conv2 = self.contract_block(32, 64)\n",
    "        self.conv3 = self.contract_block(64, 128)\n",
    "\n",
    "        self.upconv3 = self.expand_block(128, 64, 3, 1)\n",
    "        self.upconv2 = self.expand_block(64, 32, 3, 1)\n",
    "        self.upconv1 = self.expand_block(32, out_channels, 3, 1)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # downsampling part\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "\n",
    "        upconv3 = self.upconv3(conv3)\n",
    "        upconv2 = self.upconv2(upconv3)\n",
    "        upconv1 = self.upconv1(upconv2)\n",
    "\n",
    "        out = self.out(upconv1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels):\n",
    "        contract = nn.Sequential(\n",
    "            depthwiseSeparableConv(in_channels, out_channels),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        return contract\n",
    "\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "        expand = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "        )\n",
    "        return expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd69655",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = C_UNet(4, 2)\n",
    "model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "def accuracy(predb, yb):\n",
    "    return (predb.argmax(dim=1) == yb).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "\n",
    "    # Epoch metrics\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    valid_acc = []\n",
    "\n",
    "    # Load training batch\n",
    "    for images, targets in tqdm(train_loader):\n",
    "        # Send to device\n",
    "        images = images.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Loss\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, targets)\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Back Propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Scheduler\n",
    "        # lr_scheduler.step()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # EVALUATION\n",
    "    model.eval()\n",
    "\n",
    "    # Load validation batch\n",
    "    for images, targets in tqdm(valid_loader):\n",
    "        with torch.no_grad():\n",
    "            # Send to device\n",
    "            images = images.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # Loss\n",
    "            predictions = model(images)\n",
    "            loss = criterion(predictions, targets)\n",
    "            valid_loss.append(loss.item())\n",
    "            \n",
    "            # Accuracy\n",
    "            valid_acc.append(accuracy(predictions, targets))\n",
    "\n",
    "    # Log\n",
    "    train_losses.append(np.mean(train_loss))\n",
    "    valid_losses.append(np.mean(valid_loss))\n",
    "    valid_accuracies.append(np.mean(valid_acc))\n",
    "    print(f'Epoch [{epoch + 1}], train_loss: {train_losses[-1]}, val_loss: {valid_losses[-1]}, val_acc: {valid_accuracies[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4304a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = []\n",
    "test_loss = []\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Load testing batch\n",
    "for images, targets in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        # Send to device\n",
    "        images = images.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Loss\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, targets)\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        # Accuracy\n",
    "        test_accuracies.append(accuracy(predictions, targets))\n",
    "\n",
    "# Log\n",
    "print(f'Model Evaluation, Loss: {np.mean(test_loss)}, Accuracy: {np.mean(test_accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f81fd",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4601fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./models/model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a37b5",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb72617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = torch.jit.load(\"../models/model.pth\", map_location='cpu')\n",
    "model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45efa0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 4, 384, 384)\n",
    "\n",
    "torch.onnx.export(model, \n",
    "                  dummy_input, \n",
    "                  \"../models/model.onnx\", \n",
    "                  export_params=True, \n",
    "                  input_names=[\"input\"], \n",
    "                  output_names=[\"output\"],\n",
    "                  dynamic_axes={\"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "                                \"output\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5549394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(pred):\n",
    "    mask_pred = np.zeros([384,384,3])\n",
    "    mask_pred[:,:,0] = pred[:,0,:,:] * 255\n",
    "    mask_pred[:,:,1] = pred[:,1,:,:] * 255\n",
    "    return mask_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bb243",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    test_dataset[0],\n",
    "    test_dataset[1],\n",
    "    test_dataset[5],\n",
    "    test_dataset[6]\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(3, len(images), figsize=(18, 10))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for image in images:\n",
    "    # Retrieve and format results\n",
    "    display_im = image[0].permute(1, 2, 0).cpu().detach().numpy()\n",
    "    pred = model(image[0].unsqueeze(0)).cpu().detach().numpy()\n",
    "    pred = get_mask(pred)\n",
    "    mask = image[1]\n",
    "    \n",
    "    # Display\n",
    "    axs[0][i].imshow(display_im)\n",
    "    axs[0][i].set_title('Image')\n",
    "    axs[1][i].imshow(mask)\n",
    "    axs[1][i].set_title('Ground Truth')\n",
    "    axs[2][i].imshow(pred)\n",
    "    axs[2][i].set_title('Predicted Mask')\n",
    "    \n",
    "    # Increment count\n",
    "    i += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9d1ad",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1214e",
   "metadata": {},
   "source": [
    "#### Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a08cbd",
   "metadata": {},
   "source": [
    "#### Number of FLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4998a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input\n",
    "inp = torch.rand(1, 4, 384, 384)\n",
    "\n",
    "# Count the number of FLOPs\n",
    "count_ops(model, inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59069db",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824a696",
   "metadata": {},
   "source": [
    "### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b426f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_accuracies)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy by epoch');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c3e61",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"training\")\n",
    "plt.plot(valid_losses, label=\"validation\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Losses by epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a19255f787a6cc652b4d0a1e2bea561a9d91dd69cbdbda30c9cbfc7f2deccdd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
